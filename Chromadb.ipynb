{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc3f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드\n",
    "# pdf ver.\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pdf_paths = [\n",
    "            \"저작권법(법률)(제20358호)(20240828).pdf\", \n",
    "            \"저작권상담사례집2024 (3).pdf\", \n",
    "            \"인공지능과 저작권 제1-2부.pdf\", \n",
    "            \"최진원_알기 쉬운 저작권 계약 가이드북(제2판)_2024.pdf\", \n",
    "            \"naver.pdf\",\n",
    "            \"1인 미디어 창작자를 위한 저작권 안내서(2019).pdf\",\n",
    "            \"US_copyright.pdf\",\n",
    "            \"wipo_copyright.pdf\",\n",
    "            \"공공저작물 저작권 관리 및 이용 지침 해설서(개정20240101업로드용).pdf\",\n",
    "            \"네이버 블로그.pdf\",\n",
    "            \"카카오 서비스 약관20230109.pdf\",\n",
    "            \"하버드)해외 저작권, 공정이용 가이드라인.pdf\",\n",
    "            \"생성형AI 저작권 가이드라인.pdf\"\n",
    "            ]\n",
    "\n",
    "csv_paths = [\n",
    "    \"kakao_page.csv\", \n",
    "    \"kakao_policy_full_text.csv\",\n",
    "    \"kakao_rights_info.csv\",  \n",
    "    \"youtube_copyright_tools.csv\"\n",
    "]\n",
    "\n",
    "# ✅ PDF에서 블록 추출\n",
    "def extract_pdf_blocks(pdf_paths: list[str]) -> list[dict]:\n",
    "    all_blocks = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page in doc:\n",
    "            blocks_data = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for block in blocks_data:\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                text = \"\"\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text += span[\"text\"]\n",
    "                text = text.strip()\n",
    "                if text:\n",
    "                    if text.startswith(\"제\") and \"조\" in text:\n",
    "                        all_blocks.append({\"type\": \"section\", \"text\": f\"## {text}\", \"source\": os.path.basename(pdf_path)})\n",
    "                    elif text.endswith(\"가이드\") or len(text) < 20:\n",
    "                        all_blocks.append({\"type\": \"title\", \"text\": f\"# {text}\", \"source\": os.path.basename(pdf_path)})\n",
    "                    else:\n",
    "                        all_blocks.append({\"type\": \"paragraph\", \"text\": text, \"source\": os.path.basename(pdf_path)})\n",
    "    return all_blocks\n",
    "\n",
    "# ✅ CSV에서 블록 추출 (content 컬럼을 기준으로)\n",
    "def extract_csv_blocks(csv_paths: list[str], content_column: str = None) -> list[dict]:\n",
    "    all_blocks = []\n",
    "    for csv_path in csv_paths:\n",
    "        try:\n",
    "            if content_column:  # 명시된 컬럼이 있을 때\n",
    "                df = pd.read_csv(csv_path)\n",
    "                for _, row in df.iterrows():\n",
    "                    text = str(row[content_column]).strip()\n",
    "                    if text:\n",
    "                        all_blocks.append({\"type\": \"paragraph\", \"text\": text, \"source\": os.path.basename(csv_path)})\n",
    "            else:  # 컬럼 이름이 없을 경우\n",
    "                df = pd.read_csv(csv_path, header=None)\n",
    "                for _, row in df.iterrows():\n",
    "                    text = str(row[0]).strip()\n",
    "                    if text:\n",
    "                        all_blocks.append({\"type\": \"paragraph\", \"text\": text, \"source\": os.path.basename(csv_path)})\n",
    "        except Exception as e:\n",
    "            print(f\"[오류] {csv_path} 처리 중 에러 발생: {e}\")\n",
    "    return all_blocks\n",
    "\n",
    "\n",
    "# ✅ PDF + CSV 통합\n",
    "def extract_all_blocks(pdf_paths: list[str], csv_paths: list[str]) -> list[dict]:\n",
    "    pdf_blocks = extract_pdf_blocks(pdf_paths)\n",
    "    csv_blocks = extract_csv_blocks(csv_paths)\n",
    "    return pdf_blocks + csv_blocks\n",
    "\n",
    "    return all_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blocks = extract_all_blocks(pdf_paths, csv_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e0e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'section',\n",
       " 'text': '## 제2조(정의) 이 법에서 사용하는 용어의 뜻은 다음과 같다. <개정 2009. 4. 22., 2011. 6. 30., 2011. 12. 2., 2016. 3. 22.,',\n",
       " 'source': '저작권법(법률)(제20358호)(20240828).pdf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_blocks[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 1. Markdown header 기반 splitter 정의\n",
    "header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"title\"), (\"##\", \"section\")])\n",
    "\n",
    "# 2. 블록 → Markdown 텍스트로 변환\n",
    "def blocks_to_markdown_text(blocks):\n",
    "    return \"\\n\\n\".join(b[\"text\"] for b in blocks)\n",
    "\n",
    "markdown_text = blocks_to_markdown_text(all_blocks)\n",
    "\n",
    "# 3. Markdown header 기준으로 구조 단위로 분할 (Document 객체 반환됨)\n",
    "structured_chunks = header_splitter.split_text(markdown_text)\n",
    "\n",
    "# 4. Recursive splitter 설정\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "\n",
    "# 5. 구조 단위 chunk들을 다시 세부적으로 쪼개고, metadata 유지\n",
    "final_split_docs = []\n",
    "for doc in structured_chunks:\n",
    "    content = doc.page_content      # ✅ Document 객체로부터 텍스트 추출\n",
    "    metadata = doc.metadata         # ✅ Document 객체로부터 메타데이터 추출\n",
    "\n",
    "    small_chunks = recursive_splitter.split_text(content)\n",
    "    for chunk in small_chunks:\n",
    "        final_split_docs.append(Document(page_content=chunk, metadata=metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe835ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def load_json_to_documents(json_path: str, text_key: str, metadata_keys: list[str] = []) -> list[Document]:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    documents = []\n",
    "    for item in data:\n",
    "        content = item.get(text_key, \"\")\n",
    "        metadata = {k: item.get(k, \"\") for k in metadata_keys}\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def load_multiple_json_to_documents(json_paths: List[str], text_key: str, metadata_keys: List[str] = []) -> List[Document]:\n",
    "    all_documents = []\n",
    "    for path in json_paths:\n",
    "        docs = load_json_to_documents(path, text_key, metadata_keys)\n",
    "        all_documents.extend(docs)\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_paths = [\"instagram_faq_answers.json\", \"youtube_support_faq.json\"]\n",
    "\n",
    "json_docs = load_multiple_json_to_documents(  \n",
    "    json_paths, \n",
    "    text_key=\"content\", \n",
    "    metadata_keys=[\"title\", \"source\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb990192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_metadata(docs: list[Document]) -> list[Document]:\n",
    "    result = []\n",
    "    for doc in docs:\n",
    "        text = doc.page_content.lower()\n",
    "        metadata = doc.metadata.copy()\n",
    "\n",
    "        # 플랫폼\n",
    "        platforms = [\"네이버\", \"카카오\", \"유튜브\", \"인스타그램\", \"naver\", \"kakao\", \"youtube\", \"instagram\"]\n",
    "        matched_platforms = sorted({p for p in platforms if p.lower() in text})\n",
    "        if matched_platforms:\n",
    "            metadata[\"platform\"] = \", \".join(matched_platforms)\n",
    "\n",
    "        # 법 영역\n",
    "        if any(w in text for w in [\"fair use\", \"dmca\", \"united states\", \"미국\"]):\n",
    "            metadata[\"law_scope\"] = \"해외\"\n",
    "        elif any(w in text for w in [\"저작권법\", \"공공누리\", \"kogl\", \"대한민국\"]):\n",
    "            metadata[\"law_scope\"] = \"국내\"\n",
    "\n",
    "        # 문서 유형\n",
    "        if any(w in text for w in [\"사례\", \"faq\"]):\n",
    "            metadata[\"doc_type\"] = \"사례집\"\n",
    "        elif any(w in text for w in [\"가이드\", \"guide\"]):\n",
    "            metadata[\"doc_type\"] = \"가이드\"\n",
    "        elif any(w in text for w in [\"법\", \"조항\", \"제\"]):\n",
    "            metadata[\"doc_type\"] = \"법령\"\n",
    "\n",
    "        # 출처\n",
    "        if \"저작권법\" in text:\n",
    "            metadata[\"source\"] = \"저작권법\"\n",
    "        elif \"dmca\" in text:\n",
    "            metadata[\"source\"] = \"DMCA\"\n",
    "        elif \"공공누리\" in text or \"kogl\" in text:\n",
    "            metadata[\"source\"] = \"KOGL\"\n",
    "        elif \"크리에이티브 커먼즈\" in text or \"creative commons\" in text:\n",
    "            metadata[\"source\"] = \"CC\"\n",
    "\n",
    "        # 토픽\n",
    "        keyword_to_topic = {\n",
    "            \"음악\": \"음악사용\", \"배경음악\": \"음악사용\", \"이미지\": \"이미지사용\",\n",
    "            \"ai\": \"ai저작권\", \"인공지능\": \"ai저작권\", \"공정이용\": \"공정이용\",\n",
    "            \"인용\": \"인용\", \"계약\": \"저작권계약\", \"저작권료\": \"저작권계약\",\n",
    "            \"공공저작물\": \"공공저작물\"\n",
    "        }\n",
    "        topics = {tag for kw, tag in keyword_to_topic.items() if kw in text}\n",
    "        if topics:\n",
    "            metadata[\"topic\"] = \", \".join(sorted(topics))\n",
    "\n",
    "        doc.metadata = metadata\n",
    "        result.append(doc)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df182e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffce2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added batch 1/12 (size: 500)\n",
      "✅ Added batch 2/12 (size: 500)\n",
      "✅ Added batch 3/12 (size: 500)\n",
      "✅ Added batch 4/12 (size: 500)\n",
      "✅ Added batch 5/12 (size: 500)\n",
      "✅ Added batch 6/12 (size: 500)\n",
      "✅ Added batch 7/12 (size: 500)\n",
      "✅ Added batch 8/12 (size: 500)\n",
      "✅ Added batch 9/12 (size: 500)\n",
      "✅ Added batch 10/12 (size: 500)\n",
      "✅ Added batch 11/12 (size: 500)\n",
      "✅ Added batch 12/12 (size: 180)\n"
     ]
    }
   ],
   "source": [
    "# 태깅 및 통합\n",
    "result_docs = assign_metadata(final_split_docs)\n",
    "json_docs = assign_metadata(json_docs)\n",
    "\n",
    "total_docs = result_docs +  json_docs\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "import math\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# 1. Chroma 인스턴스 생성\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    collection_name=\"rag_chatbot\",\n",
    "    persist_directory=\"vector_store/chroma/rag_chatbot\"\n",
    ")\n",
    "\n",
    "# # 2. 문서 배치 추가 함수 정의\n",
    "def batch_add_documents(vector_store, documents: list[Document], batch_size: int = 500):\n",
    "    total = len(documents)\n",
    "    num_batches = math.ceil(total / batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch = documents[i * batch_size : (i + 1) * batch_size]\n",
    "        vector_store.add_documents(batch)\n",
    "        print(f\"✅ Added batch {i+1}/{num_batches} (size: {len(batch)})\")\n",
    "\n",
    "batch_add_documents(vector_store, total_docs, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff2b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ page_content 있는 문서 수: 5649\n"
     ]
    }
   ],
   "source": [
    "filtered_docs = [doc for doc in total_docs if doc.page_content.strip() != \"\"]\n",
    "print(f\"✅ page_content 있는 문서 수: {len(filtered_docs)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
